import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR
from torchmetrics import Accuracy, F1Score, ConfusionMatrix
import pytorchvideo.models as pv_models
from pytorchvideo.models.accelerator.mobile_cpu.efficient_x3d import create_x3d
from pytorchvideo.accelerator.deployment.mobile_cpu.utils.model_conversion import (
    convert_to_deployable_form,
)
from typing import Dict, Any, Optional


class SignLanguageClassifier(pl.LightningModule):
    """
    ìˆ˜ì–´ ë¶„ë¥˜ë¥¼ ìœ„í•œ PyTorch Lightning ëª¨ë¸
    """
    
    def __init__(
        self,
        num_classes: int,
        model_name: str = "slow_r50",
        learning_rate: float = 1e-3,
        optimizer: str = "adam",
        scheduler: str = "cosine",
        pretrained: bool = True,
        freeze_backbone: bool = False,
        dropout_rate: float = 0.5,
        label_smoothing: float = 0.0,
        weight_decay: float = 1e-4,
        enable_efficient_deployment: bool = False,
        efficient_input_size: tuple = (1, 3, 4, 160, 160),
        **kwargs
    ):
        """
        Args:
            num_classes: í´ëž˜ìŠ¤ ê°œìˆ˜
            model_name: ëª¨ë¸ ì´ë¦„ (slow_r50, x3d_m, mvit_base_16x4, efficient_x3d_xs, efficient_x3d_s ë“±)
            learning_rate: í•™ìŠµë¥ 
            optimizer: ì˜µí‹°ë§ˆì´ì € (adam, sgd)
            scheduler: ìŠ¤ì¼€ì¤„ëŸ¬ (cosine, step, none)
            pretrained: ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
            freeze_backbone: ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ê³ ì • ì—¬ë¶€
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            label_smoothing: ë¼ë²¨ ìŠ¤ë¬´ë”©
            weight_decay: ê°€ì¤‘ì¹˜ ê°ì‡ 
            enable_efficient_deployment: íš¨ìœ¨ì  ë°°í¬ ëª¨ë“œ í™œì„±í™” (ì¶”ë¡  ì‹œ ìžë™ ë³€í™˜)
            efficient_input_size: íš¨ìœ¨ì  ëª¨ë¸ ë³€í™˜ì„ ìœ„í•œ ìž…ë ¥ í¬ê¸°
        """
        super().__init__()
        self.save_hyperparameters()
        
        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.optimizer_name = optimizer
        self.scheduler_name = scheduler
        self.weight_decay = weight_decay
        self.label_smoothing = label_smoothing
        self.enable_efficient_deployment = enable_efficient_deployment
        self.efficient_input_size = efficient_input_size
        self.is_efficient_model = model_name.startswith("efficient_")
        self.deployed_model = None
        
        # ëª¨ë¸ ìƒì„±
        self.model = self._create_model(
            model_name, num_classes, pretrained, freeze_backbone, dropout_rate
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
        
        # ë©”íŠ¸ë¦­
        self.train_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        self.val_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        self.test_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        
        self.val_f1 = F1Score(task="multiclass", num_classes=num_classes, average="macro")
        self.test_f1 = F1Score(task="multiclass", num_classes=num_classes, average="macro")
        
        # í˜¼ë™ í–‰ë ¬ (í…ŒìŠ¤íŠ¸ ì‹œì—ë§Œ ì‚¬ìš©)
        self.test_confusion_matrix = ConfusionMatrix(task="multiclass", num_classes=num_classes)
    
    def _create_model(self, model_name: str, num_classes: int, pretrained: bool, 
                     freeze_backbone: bool, dropout_rate: float) -> nn.Module:
        """ëª¨ë¸ ìƒì„±"""
        
        # PyTorchVideo ëª¨ë¸ ìƒì„±
        if model_name == "slow_r50":
            model = pv_models.slow_r50(pretrained=pretrained)
            if hasattr(model, 'blocks') and hasattr(model.blocks[-1], 'proj'):
                in_features = model.blocks[-1].proj.in_features
                model.blocks[-1].proj = nn.Sequential(
                    nn.Dropout(dropout_rate),
                    nn.Linear(in_features, num_classes)
                )
        elif model_name == "x3d_m":
            model = pv_models.x3d_m(pretrained=pretrained)
            if hasattr(model, 'blocks') and hasattr(model.blocks[-1], 'proj'):
                in_features = model.blocks[-1].proj.in_features
                model.blocks[-1].proj = nn.Sequential(
                    nn.Dropout(dropout_rate),
                    nn.Linear(in_features, num_classes)
                )
        elif model_name == "x3d_s":
            model = pv_models.x3d_s(pretrained=pretrained)
            if hasattr(model, 'blocks') and hasattr(model.blocks[-1], 'proj'):
                in_features = model.blocks[-1].proj.in_features
                model.blocks[-1].proj = nn.Sequential(
                    nn.Dropout(dropout_rate),
                    nn.Linear(in_features, num_classes)
                )
        elif model_name == "mvit_base_16x4":
            model = pv_models.mvit_base_16x4(pretrained=pretrained)
            if hasattr(model, 'head') and hasattr(model.head, 'proj'):
                in_features = model.head.proj.in_features
                model.head.proj = nn.Sequential(
                    nn.Dropout(dropout_rate),
                    nn.Linear(in_features, num_classes)
                )
        elif model_name == "efficient_x3d_xs":
            model = create_x3d(
                num_classes=num_classes,
                dropout=dropout_rate,
                expansion="XS",
                head_act="relu",
                enable_head=True,
            )
            # ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„ íƒì‚¬í•­)
            if pretrained:
                try:
                    from torch.hub import load_state_dict_from_url
                    checkpoint_url = "https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/efficient_x3d_xs_original_form.pyth"
                    state_dict = load_state_dict_from_url(checkpoint_url, map_location="cpu")
                    # ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ëŠ” í´ëž˜ìŠ¤ ìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ì œì™¸
                    filtered_state_dict = {k: v for k, v in state_dict.items() 
                                         if not k.startswith('projection')}
                    model.load_state_dict(filtered_state_dict, strict=False)
                except:
                    print("Warning: Could not load pretrained weights for efficient_x3d_xs")
        elif model_name == "efficient_x3d_s":
            model = create_x3d(
                num_classes=num_classes,
                dropout=dropout_rate,
                expansion="S",
                head_act="relu",
                enable_head=True,
            )
            # ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„ íƒì‚¬í•­)
            if pretrained:
                try:
                    from torch.hub import load_state_dict_from_url
                    checkpoint_url = "https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/efficient_x3d_s_original_form.pyth"
                    state_dict = load_state_dict_from_url(checkpoint_url, map_location="cpu")
                    # ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ëŠ” í´ëž˜ìŠ¤ ìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ì œì™¸
                    filtered_state_dict = {k: v for k, v in state_dict.items() 
                                         if not k.startswith('projection')}
                    model.load_state_dict(filtered_state_dict, strict=False)
                except:
                    print("Warning: Could not load pretrained weights for efficient_x3d_s")
        else:
            raise ValueError(f"Unsupported model: {model_name}")
        
        # ë°±ë³¸ ê³ ì •
        if freeze_backbone:
            for name, param in model.named_parameters():
                if "proj" not in name and "head" not in name:
                    param.requires_grad = False
        
        return model
    
    def forward(self, x):
        # íš¨ìœ¨ì  ë°°í¬ ëª¨ë“œê°€ í™œì„±í™”ë˜ê³  ì¶”ë¡  ëª¨ë“œì¼ ë•Œ
        if (self.enable_efficient_deployment and 
            not self.training and 
            self.is_efficient_model and 
            self.deployed_model is not None):
            return self.deployed_model(x)
        return self.model(x)
    
    def convert_to_deployment_mode(self):
        """
        íš¨ìœ¨ì  ëª¨ë¸ì„ ë°°í¬ ëª¨ë“œë¡œ ë³€í™˜
        ì´ ë©”ì„œë“œëŠ” ì¶”ë¡  ì „ì— í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        if self.is_efficient_model and self.enable_efficient_deployment:
            try:
                # ì˜ˆì‹œ ìž…ë ¥ í…ì„œ ìƒì„±
                input_tensor = torch.randn(self.efficient_input_size)
                if next(self.parameters()).is_cuda:
                    input_tensor = input_tensor.cuda()
                
                # ë°°í¬ ëª¨ë“œë¡œ ë³€í™˜
                self.deployed_model = convert_to_deployable_form(self.model, input_tensor)
                print("âœ… ëª¨ë¸ì´ íš¨ìœ¨ì  ë°°í¬ ëª¨ë“œë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            except Exception as e:
                print(f"âš ï¸ ë°°í¬ ëª¨ë“œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return False
        return False
    
    def export_to_mobile(self, output_path: str, quantize: bool = False):
        """
        ëª¨ë°”ì¼ ë°°í¬ë¥¼ ìœ„í•œ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
        
        Args:
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (.pt í™•ìž¥ìž)
            quantize: INT8 ì–‘ìží™” ì—¬ë¶€
        """
        if not self.is_efficient_model:
            raise ValueError("ëª¨ë°”ì¼ ë‚´ë³´ë‚´ê¸°ëŠ” efficient ëª¨ë¸ì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
        
        self.eval()
        
        # ë°°í¬ ëª¨ë“œë¡œ ë³€í™˜
        if self.deployed_model is None:
            if not self.convert_to_deployment_mode():
                raise RuntimeError("ë°°í¬ ëª¨ë“œ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        input_tensor = torch.randn(self.efficient_input_size)
        if next(self.parameters()).is_cuda:
            input_tensor = input_tensor.cuda()
        
        if quantize:
            # INT8 ì–‘ìží™”
            print("ðŸ”„ INT8 ì–‘ìží™” ì§„í–‰ ì¤‘...")
            try:
                from torch.quantization import (
                    convert, prepare, get_default_qconfig, QuantStub, DeQuantStub
                )
                from torch.utils.mobile_optimizer import optimize_for_mobile
                
                # ì–‘ìží™”ë¥¼ ìœ„í•œ ëž˜í¼ í´ëž˜ìŠ¤
                class QuantWrapper(nn.Module):
                    def __init__(self, model):
                        super().__init__()
                        self.quant = QuantStub()
                        self.model = model
                        self.dequant = DeQuantStub()
                    
                    def forward(self, x):
                        x = self.quant(x)
                        x = self.model(x)
                        x = self.dequant(x)
                        return x
                
                # ì–‘ìží™” ì„¤ì •
                torch.backends.quantized.engine = "qnnpack"
                wrapped_model = QuantWrapper(self.deployed_model.cpu())
                wrapped_model.qconfig = get_default_qconfig("qnnpack")
                
                # ì–‘ìží™” ì¤€ë¹„ ë° ì‹¤í–‰
                prepared_model = prepare(wrapped_model)
                # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ëª‡ ê°œì˜ ìƒ˜í”Œ ë°ì´í„° í•„ìš”)
                with torch.no_grad():
                    prepared_model(input_tensor.cpu())
                
                quantized_model = convert(prepared_model)
                
                # TorchScriptë¡œ ë‚´ë³´ë‚´ê¸°
                traced_model = torch.jit.trace(quantized_model, input_tensor.cpu(), strict=False)
                optimized_model = optimize_for_mobile(traced_model)
                optimized_model.save(output_path)
                
                print(f"âœ… ì–‘ìží™”ëœ ëª¨ë¸ì´ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")
                
            except Exception as e:
                print(f"âŒ ì–‘ìží™” ì‹¤íŒ¨: {e}")
                print("ðŸ”„ FP32 ëª¨ë¸ë¡œ ëŒ€ì‹  ì €ìž¥í•©ë‹ˆë‹¤...")
                quantize = False
        
        if not quantize:
            # FP32 ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
            try:
                from torch.utils.mobile_optimizer import optimize_for_mobile
                
                traced_model = torch.jit.trace(self.deployed_model, input_tensor, strict=False)
                optimized_model = optimize_for_mobile(traced_model)
                optimized_model.save(output_path)
                
                print(f"âœ… FP32 ëª¨ë¸ì´ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")
                
            except Exception as e:
                print(f"âŒ ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
                raise
    
    def training_step(self, batch, batch_idx):
        """í•™ìŠµ ìŠ¤í…"""
        videos, labels = batch["video"], batch["label"]
        
        # ë¼ë²¨ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ë¬¸ìžì—´ ë¼ë²¨ì¸ ê²½ìš°)
        if isinstance(labels[0], str):
            # í´ëž˜ìŠ¤ ì´ë¦„ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            label_indices = []
            for label in labels:
                if hasattr(self.trainer.datamodule, 'class_to_idx'):
                    label_indices.append(self.trainer.datamodule.class_to_idx[label])
                else:
                    # fallback: ë¬¸ìžì—´ì„ í•´ì‹œí•˜ì—¬ ì¸ë±ìŠ¤ ìƒì„± (ìž„ì‹œ)
                    label_indices.append(hash(label) % self.num_classes)
            labels = torch.tensor(label_indices, device=self.device)
        
        # ìˆœì „íŒŒ
        logits = self(videos)
        loss = self.criterion(logits, labels)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        preds = torch.argmax(logits, dim=1)
        self.train_accuracy(preds, labels)
        
        # ë¡œê¹…
        self.log("train/loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/accuracy", self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        """ê²€ì¦ ìŠ¤í…"""
        videos, labels = batch["video"], batch["label"]
        
        # ë¼ë²¨ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        if isinstance(labels[0], str):
            label_indices = []
            for label in labels:
                if hasattr(self.trainer.datamodule, 'class_to_idx'):
                    label_indices.append(self.trainer.datamodule.class_to_idx[label])
                else:
                    label_indices.append(hash(label) % self.num_classes)
            labels = torch.tensor(label_indices, device=self.device)
        
        # ìˆœì „íŒŒ
        logits = self(videos)
        loss = self.criterion(logits, labels)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        preds = torch.argmax(logits, dim=1)
        self.val_accuracy(preds, labels)
        self.val_f1(preds, labels)
        
        # ë¡œê¹…
        self.log("val/loss", loss, on_epoch=True, prog_bar=True)
        self.log("val/accuracy", self.val_accuracy, on_epoch=True, prog_bar=True)
        self.log("val/f1", self.val_f1, on_epoch=True)
        
        return loss
    
    def test_step(self, batch, batch_idx):
        """í…ŒìŠ¤íŠ¸ ìŠ¤í…"""
        videos, labels = batch["video"], batch["label"]
        
        # ë¼ë²¨ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        if isinstance(labels[0], str):
            label_indices = []
            for label in labels:
                if hasattr(self.trainer.datamodule, 'class_to_idx'):
                    label_indices.append(self.trainer.datamodule.class_to_idx[label])
                else:
                    label_indices.append(hash(label) % self.num_classes)
            labels = torch.tensor(label_indices, device=self.device)
        
        # ìˆœì „íŒŒ
        logits = self(videos)
        loss = self.criterion(logits, labels)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        preds = torch.argmax(logits, dim=1)
        self.test_accuracy(preds, labels)
        self.test_f1(preds, labels)
        self.test_confusion_matrix(preds, labels)
        
        # ë¡œê¹…
        self.log("test/loss", loss, on_epoch=True)
        self.log("test/accuracy", self.test_accuracy, on_epoch=True)
        self.log("test/f1", self.test_f1, on_epoch=True)
        
        return loss
    
    def configure_optimizers(self):
        """ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        
        # ì˜µí‹°ë§ˆì´ì € ì„ íƒ
        if self.optimizer_name.lower() == "adam":
            optimizer = Adam(
                self.parameters(),
                lr=self.learning_rate,
                weight_decay=self.weight_decay
            )
        elif self.optimizer_name.lower() == "sgd":
            optimizer = SGD(
                self.parameters(),
                lr=self.learning_rate,
                momentum=0.9,
                weight_decay=self.weight_decay
            )
        else:
            raise ValueError(f"Unsupported optimizer: {self.optimizer_name}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„ íƒ
        if self.scheduler_name.lower() == "cosine":
            scheduler = CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs)
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "epoch",
                    "frequency": 1,
                }
            }
        elif self.scheduler_name.lower() == "step":
            scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "epoch",
                    "frequency": 1,
                }
            }
        else:
            return optimizer
    
    def on_test_epoch_end(self):
        """í…ŒìŠ¤íŠ¸ ì—í¬í¬ ì¢…ë£Œ ì‹œ í˜¼ë™ í–‰ë ¬ ì¶œë ¥"""
        confusion_matrix = self.test_confusion_matrix.compute()
        
        print("\nConfusion Matrix:")
        print(confusion_matrix)
        
        # í´ëž˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
        if hasattr(self.trainer.datamodule, 'idx_to_class'):
            class_names = [self.trainer.datamodule.idx_to_class[i] for i in range(self.num_classes)]
            
            print("\nPer-class Accuracy:")
            for i, class_name in enumerate(class_names):
                if confusion_matrix[i].sum() > 0:
                    class_acc = confusion_matrix[i, i] / confusion_matrix[i].sum()
                    print(f"{class_name}: {class_acc:.4f}") 