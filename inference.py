#!/usr/bin/env python3
"""
ìˆ˜ì–´ ë¶„ë¥˜ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš© ì˜ˆì‹œ:
python inference.py --checkpoint ./outputs/sign_language_classification/checkpoints/best.ckpt --video_path ./test_video.mp4
"""

import argparse
import json
import os
from pathlib import Path

import torch
import torch.nn.functional as F
from pytorchvideo.data.clip_sampling import UniformClipSampler
from pytorchvideo.data.encoded_video import EncodedVideo
from pytorchvideo.transforms import (
    ApplyTransformToKey,
    ShortSideScale,
    UniformTemporalSubsample,
)
from torchvision.transforms import Compose, Lambda
from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo

from sign_language_model import SignLanguageClassifier


class SignLanguageInference:
    """ìˆ˜ì–´ ë¶„ë¥˜ ì¶”ë¡  í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        checkpoint_path: str,
        device: str = "auto",
        num_frames: int = 16,
        crop_size: int = 224,
        mean: tuple = (0.45, 0.45, 0.45),
        std: tuple = (0.225, 0.225, 0.225),
    ):
        """
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            device: ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda')
            num_frames: ìƒ˜í”Œë§í•  í”„ë ˆì„ ìˆ˜
            crop_size: ì´ë¯¸ì§€ í¬ë¡­ í¬ê¸°
            mean: ì •ê·œí™” í‰ê· ê°’
            std: ì •ê·œí™” í‘œì¤€í¸ì°¨
        """
        self.checkpoint_path = checkpoint_path
        self.num_frames = num_frames
        self.crop_size = crop_size
        self.mean = mean
        self.std = std
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self._load_model()
        self.model.eval()
        
        # Efficient ëª¨ë¸ì¸ ê²½ìš° ë°°í¬ ëª¨ë“œë¡œ ë³€í™˜
        if hasattr(self.model, 'is_efficient_model') and self.model.is_efficient_model:
            print("ğŸ”„ Efficient ëª¨ë¸ì„ ë°°í¬ ëª¨ë“œë¡œ ë³€í™˜ ì¤‘...")
            self.model.convert_to_deployment_mode()
        
        # ë³€í™˜ í•¨ìˆ˜ ì„¤ì •
        self.transform = self._get_transforms()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ“Š í´ë˜ìŠ¤ ê°œìˆ˜: {self.model.num_classes}")
    
    def _load_model(self) -> SignLanguageClassifier:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {self.checkpoint_path}")
        
        model = SignLanguageClassifier.load_from_checkpoint(
            self.checkpoint_path,
            map_location=self.device
        )
        model.to(self.device)
        
        return model
    
    def _get_transforms(self):
        """ì¶”ë¡ ìš© ë°ì´í„° ë³€í™˜ í•¨ìˆ˜"""
        return Compose([
            ApplyTransformToKey(
                key="video",
                transform=Compose([
                    UniformTemporalSubsample(self.num_frames),
                    Lambda(lambda x: x / 255.0),
                    NormalizeVideo(self.mean, self.std),
                    ShortSideScale(size=256),
                    CenterCropVideo(self.crop_size),
                ]),
            ),
        ])
    
    def predict_video(
        self,
        video_path: str,
        clip_duration: float = 2.0,
        start_time: float = 0.0,
        return_probabilities: bool = False,
    ) -> dict:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì— ëŒ€í•œ ìˆ˜ì–´ ë¶„ë¥˜ ì˜ˆì¸¡
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            clip_duration: í´ë¦½ ì§€ì† ì‹œê°„ (ì´ˆ)
            start_time: ì‹œì‘ ì‹œê°„ (ì´ˆ)
            return_probabilities: í™•ë¥ ê°’ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {video_path}")
        
        # ë¹„ë””ì˜¤ ë¡œë“œ
        video = EncodedVideo.from_path(video_path)
        
        # í´ë¦½ ìƒ˜í”Œë§
        end_time = start_time + clip_duration
        if end_time > video.duration:
            end_time = video.duration
            start_time = max(0, end_time - clip_duration)
        
        # ë¹„ë””ì˜¤ í´ë¦½ ì¶”ì¶œ
        video_data = video.get_clip(start_sec=start_time, end_sec=end_time)
        
        # ë³€í™˜ ì ìš©
        video_data = self.transform(video_data)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        video_tensor = video_data["video"].unsqueeze(0).to(self.device)
        
        # ì¶”ë¡ 
        with torch.no_grad():
            logits = self.model(video_tensor)
            probabilities = F.softmax(logits, dim=1)
            predicted_idx = torch.argmax(logits, dim=1).item()
            confidence = probabilities[0, predicted_idx].item()
        
        # ê²°ê³¼ ìƒì„±
        result = {
            "video_path": video_path,
            "start_time": start_time,
            "end_time": end_time,
            "predicted_class_idx": predicted_idx,
            "confidence": confidence,
            "clip_duration": end_time - start_time,
        }
        
        # í´ë˜ìŠ¤ ì´ë¦„ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
        if hasattr(self.model, 'hparams') and hasattr(self.model.hparams, 'class_names'):
            class_names = self.model.hparams.class_names
            if predicted_idx < len(class_names):
                result["predicted_class_name"] = class_names[predicted_idx]
        
        # í™•ë¥ ê°’ ì¶”ê°€ (ìš”ì²­ëœ ê²½ìš°)
        if return_probabilities:
            result["probabilities"] = probabilities[0].cpu().numpy().tolist()
        
        return result
    
    def predict_multiple_clips(
        self,
        video_path: str,
        clip_duration: float = 2.0,
        stride: float = 1.0,
        return_probabilities: bool = False,
    ) -> list:
        """
        ë¹„ë””ì˜¤ì—ì„œ ì—¬ëŸ¬ í´ë¦½ì— ëŒ€í•œ ì˜ˆì¸¡
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            clip_duration: í´ë¦½ ì§€ì† ì‹œê°„ (ì´ˆ)
            stride: í´ë¦½ ê°„ê²© (ì´ˆ)
            return_probabilities: í™•ë¥ ê°’ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ë‹¤ì¤‘ í´ë¦½ ì²˜ë¦¬ ì¤‘: {video_path}")
        
        # ë¹„ë””ì˜¤ ë¡œë“œ
        video = EncodedVideo.from_path(video_path)
        
        results = []
        start_time = 0.0
        
        while start_time + clip_duration <= video.duration:
            result = self.predict_video(
                video_path,
                clip_duration=clip_duration,
                start_time=start_time,
                return_probabilities=return_probabilities,
            )
            results.append(result)
            start_time += stride
        
        return results


def parse_args():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="ìˆ˜ì–´ ë¶„ë¥˜ ëª¨ë¸ ì¶”ë¡ ")
    
    parser.add_argument("--checkpoint", type=str, required=True,
                       help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--video_path", type=str, required=True,
                       help="ì¶”ë¡ í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_path", type=str, default=None,
                       help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (JSON íŒŒì¼)")
    
    # ì¶”ë¡  ì„¤ì •
    parser.add_argument("--clip_duration", type=float, default=2.0,
                       help="í´ë¦½ ì§€ì† ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--start_time", type=float, default=0.0,
                       help="ì‹œì‘ ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--multiple_clips", action="store_true",
                       help="ì—¬ëŸ¬ í´ë¦½ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰")
    parser.add_argument("--stride", type=float, default=1.0,
                       help="í´ë¦½ ê°„ê²© (ì´ˆ, multiple_clips ëª¨ë“œ)")
    parser.add_argument("--return_probabilities", action="store_true",
                       help="í™•ë¥ ê°’ í¬í•¨í•˜ì—¬ ë°˜í™˜")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--device", type=str, default="auto",
                       choices=["auto", "cpu", "cuda"],
                       help="ë””ë°”ì´ìŠ¤")
    parser.add_argument("--num_frames", type=int, default=16,
                       help="ìƒ˜í”Œë§í•  í”„ë ˆì„ ìˆ˜")
    parser.add_argument("--crop_size", type=int, default=224,
                       help="ì´ë¯¸ì§€ í¬ë¡­ í¬ê¸°")
    
    return parser.parse_args()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_args()
    
    print("ğŸš€ ìˆ˜ì–´ ë¶„ë¥˜ ì¶”ë¡  ì‹œì‘")
    print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸: {args.checkpoint}")
    print(f"ğŸ¬ ë¹„ë””ì˜¤: {args.video_path}")
    
    # ì¶”ë¡ ê¸° ì´ˆê¸°í™”
    inference = SignLanguageInference(
        checkpoint_path=args.checkpoint,
        device=args.device,
        num_frames=args.num_frames,
        crop_size=args.crop_size,
    )
    
    # ì¶”ë¡  ìˆ˜í–‰
    if args.multiple_clips:
        print(f"ğŸ”„ ë‹¤ì¤‘ í´ë¦½ ëª¨ë“œ (stride: {args.stride}ì´ˆ)")
        results = inference.predict_multiple_clips(
            video_path=args.video_path,
            clip_duration=args.clip_duration,
            stride=args.stride,
            return_probabilities=args.return_probabilities,
        )
        
        print(f"\nğŸ“Š ì´ {len(results)}ê°œ í´ë¦½ ì²˜ë¦¬ ì™„ë£Œ")
        for i, result in enumerate(results):
            print(f"í´ë¦½ {i+1}: {result['start_time']:.1f}s-{result['end_time']:.1f}s")
            print(f"   ì˜ˆì¸¡: í´ë˜ìŠ¤ {result['predicted_class_idx']} (ì‹ ë¢°ë„: {result['confidence']:.4f})")
            if 'predicted_class_name' in result:
                print(f"   í´ë˜ìŠ¤ëª…: {result['predicted_class_name']}")
    else:
        print(f"ğŸ¯ ë‹¨ì¼ í´ë¦½ ëª¨ë“œ")
        result = inference.predict_video(
            video_path=args.video_path,
            clip_duration=args.clip_duration,
            start_time=args.start_time,
            return_probabilities=args.return_probabilities,
        )
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"   í´ë¦½ êµ¬ê°„: {result['start_time']:.1f}s - {result['end_time']:.1f}s")
        print(f"   ì˜ˆì¸¡ í´ë˜ìŠ¤: {result['predicted_class_idx']}")
        print(f"   ì‹ ë¢°ë„: {result['confidence']:.4f}")
        if 'predicted_class_name' in result:
            print(f"   í´ë˜ìŠ¤ëª…: {result['predicted_class_name']}")
        
        results = result
    
    # ê²°ê³¼ ì €ì¥
    if args.output_path:
        output_path = Path(args.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    print("âœ… ì¶”ë¡  ì™„ë£Œ!")


if __name__ == "__main__":
    main() 