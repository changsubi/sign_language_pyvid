#!/usr/bin/env python3
"""
EfficientX3D ëª¨ë¸ ê¸°ëŠ¥ ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” EfficientX3D ëª¨ë¸ì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì‹œì—°í•©ë‹ˆë‹¤:
1. ëª¨ë¸ ìƒì„± ë° ì •ë³´ ì¶œë ¥
2. ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
3. ëª¨ë°”ì¼ ë°°í¬ ëª¨ë“œ ë³€í™˜
4. INT8 ì–‘ìí™”
5. TorchScript ë‚´ë³´ë‚´ê¸°

ì‚¬ìš© ì˜ˆì‹œ:
python demo_efficient_x3d.py --variant XS
python demo_efficient_x3d.py --variant S --benchmark --export_mobile
"""

import argparse
import time
import torch
import torch.nn as nn
from pathlib import Path
import psutil
import os

from pytorchvideo.models.accelerator.mobile_cpu.efficient_x3d import create_x3d
from pytorchvideo.accelerator.deployment.mobile_cpu.utils.model_conversion import (
    convert_to_deployable_form,
)


def get_model_config(variant: str):
    """ëª¨ë¸ ë³€í˜•ë³„ ì„¤ì •"""
    configs = {
        "XS": {
            "input_size": (1, 3, 4, 160, 160),
            "expansion": "XS",
            "expected_accuracy": {"top1": 68.5, "top5": 88.0},
            "mobile_latency": {"fp32": 233, "int8": 165},
            "description": "ëª¨ë°”ì¼ ìµœì í™” - ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ìš©"
        },
        "S": {
            "input_size": (1, 3, 13, 160, 160),
            "expansion": "S", 
            "expected_accuracy": {"top1": 73.0, "top5": 90.6},
            "mobile_latency": {"fp32": 764, "int8": None},
            "description": "ê· í˜•ì¡íŒ ì„±ëŠ¥ - ì •í™•ë„ì™€ íš¨ìœ¨ì„±ì˜ ë°¸ëŸ°ìŠ¤"
        }
    }
    return configs.get(variant, configs["XS"])


def create_demo_model(variant: str, num_classes: int = 400):
    """ë°ëª¨ìš© EfficientX3D ëª¨ë¸ ìƒì„±"""
    config = get_model_config(variant)
    
    model = create_x3d(
        num_classes=num_classes,
        dropout=0.5,
        expansion=config["expansion"],
        head_act="relu",
        enable_head=True,
    )
    
    return model, config


def benchmark_model(model, input_tensor, num_runs: int = 100, warmup_runs: int = 10):
    """ëª¨ë¸ ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
    model.eval()
    device = next(model.parameters()).device
    input_tensor = input_tensor.to(device)
    
    # Warmup
    print(f"â³ Warmup ì¤‘... ({warmup_runs} runs)")
    with torch.no_grad():
        for _ in range(warmup_runs):
            _ = model(input_tensor)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    if device.type == 'cuda':
        torch.cuda.synchronize()
        memory_before = torch.cuda.memory_allocated() / 1024**2  # MB
    else:
        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss / 1024**2  # MB
    
    # ë²¤ì¹˜ë§ˆí‚¹
    print(f"ğŸ”¥ ë²¤ì¹˜ë§ˆí‚¹ ì¤‘... ({num_runs} runs)")
    times = []
    
    with torch.no_grad():
        for i in range(num_runs):
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            start_time = time.time()
            output = model(input_tensor)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
            
            if (i + 1) % 20 == 0:
                print(f"  ì§„í–‰ë¥ : {i+1}/{num_runs}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    if device.type == 'cuda':
        memory_after = torch.cuda.memory_allocated() / 1024**2  # MB
        memory_used = memory_after - memory_before
    else:
        process = psutil.Process(os.getpid())
        memory_after = process.memory_info().rss / 1024**2  # MB
        memory_used = memory_after - memory_before
    
    return {
        'mean_time': sum(times) / len(times),
        'min_time': min(times),
        'max_time': max(times),
        'std_time': (sum([(t - sum(times) / len(times))**2 for t in times]) / len(times))**0.5,
        'memory_used': memory_used,
        'output_shape': output.shape
    }


def quantize_model(model, input_tensor):
    """ëª¨ë¸ INT8 ì–‘ìí™”"""
    print("ğŸ”„ INT8 ì–‘ìí™” ì§„í–‰ ì¤‘...")
    
    try:
        from torch.quantization import (
            convert, prepare, get_default_qconfig, QuantStub, DeQuantStub
        )
        
        # ì–‘ìí™”ë¥¼ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
        class QuantWrapper(nn.Module):
            def __init__(self, model):
                super().__init__()
                self.quant = QuantStub()
                self.model = model
                self.dequant = DeQuantStub()
            
            def forward(self, x):
                x = self.quant(x)
                x = self.model(x)
                x = self.dequant(x)
                return x
        
        # ì–‘ìí™” ì„¤ì •
        torch.backends.quantized.engine = "qnnpack"
        wrapped_model = QuantWrapper(model.cpu())
        wrapped_model.qconfig = get_default_qconfig("qnnpack")
        
        # ì–‘ìí™” ì¤€ë¹„ ë° ì‹¤í–‰
        prepared_model = prepare(wrapped_model)
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
        with torch.no_grad():
            prepared_model(input_tensor.cpu())
        
        quantized_model = convert(prepared_model)
        
        print("âœ… ì–‘ìí™” ì™„ë£Œ!")
        return quantized_model
        
    except Exception as e:
        print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
        return None


def export_mobile_models(model, input_tensor, variant: str, output_dir: str = "./mobile_models"):
    """ëª¨ë°”ì¼ ë°°í¬ìš© ëª¨ë¸ ë‚´ë³´ë‚´ê¸°"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print("\nğŸ“± ëª¨ë°”ì¼ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°")
    
    # 1. FP32 ë°°í¬ ëª¨ë¸
    try:
        print("ğŸ”„ FP32 ë°°í¬ ëª¨ë¸ ë³€í™˜ ì¤‘...")
        deployed_model = convert_to_deployable_form(model, input_tensor)
        
        from torch.utils.mobile_optimizer import optimize_for_mobile
        traced_model = torch.jit.trace(deployed_model, input_tensor, strict=False)
        optimized_model = optimize_for_mobile(traced_model)
        
        fp32_path = output_dir / f"efficient_x3d_{variant.lower()}_fp32.pt"
        optimized_model.save(str(fp32_path))
        print(f"âœ… FP32 ëª¨ë¸ ì €ì¥: {fp32_path}")
        
        # 2. INT8 ì–‘ìí™” ëª¨ë¸
        print("ğŸ”„ INT8 ì–‘ìí™” ëª¨ë¸ ìƒì„± ì¤‘...")
        quantized_model = quantize_model(deployed_model, input_tensor)
        
        if quantized_model is not None:
            traced_quant = torch.jit.trace(quantized_model, input_tensor.cpu(), strict=False)
            optimized_quant = optimize_for_mobile(traced_quant)
            
            int8_path = output_dir / f"efficient_x3d_{variant.lower()}_int8.pt"
            optimized_quant.save(str(int8_path))
            print(f"âœ… INT8 ëª¨ë¸ ì €ì¥: {int8_path}")
            
            return fp32_path, int8_path
        else:
            return fp32_path, None
            
    except Exception as e:
        print(f"âŒ ëª¨ë°”ì¼ ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        return None, None


def main():
    parser = argparse.ArgumentParser(description="EfficientX3D ëª¨ë¸ ê¸°ëŠ¥ ë°ëª¨")
    parser.add_argument("--variant", type=str, default="XS", choices=["XS", "S"],
                       help="EfficientX3D ëª¨ë¸ ë³€í˜•")
    parser.add_argument("--num_classes", type=int, default=400,
                       help="ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜")
    parser.add_argument("--benchmark", action="store_true",
                       help="ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ìˆ˜í–‰")
    parser.add_argument("--num_runs", type=int, default=100,
                       help="ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰ íšŸìˆ˜")
    parser.add_argument("--export_mobile", action="store_true",
                       help="ëª¨ë°”ì¼ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°")
    parser.add_argument("--device", type=str, default="auto",
                       choices=["auto", "cpu", "cuda"],
                       help="ì‹¤í–‰ ë””ë°”ì´ìŠ¤")
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    
    print(f"ğŸš€ EfficientX3D-{args.variant} ëª¨ë¸ ë°ëª¨")
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ¯ í´ë˜ìŠ¤ ìˆ˜: {args.num_classes}")
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ¤– EfficientX3D-{args.variant} ëª¨ë¸ ìƒì„± ì¤‘...")
    model, config = create_demo_model(args.variant, args.num_classes)
    model = model.to(device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"   - ì„¤ëª…: {config['description']}")
    print(f"   - ì…ë ¥ í¬ê¸°: {config['input_size']}")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    expected_acc = config['expected_accuracy']
    print(f"   - ì˜ˆìƒ ì„±ëŠ¥ (Kinetics-400): Top-1 {expected_acc['top1']}%, Top-5 {expected_acc['top5']}%")
    
    mobile_lat = config['mobile_latency']
    print(f"   - ëª¨ë°”ì¼ ì§€ì—°ì‹œê°„ (Samsung S8): FP32 {mobile_lat['fp32']}ms", end="")
    if mobile_lat['int8']:
        print(f", INT8 {mobile_lat['int8']}ms")
    else:
        print()
    
    # ì…ë ¥ í…ì„œ ìƒì„±
    input_tensor = torch.randn(config['input_size']).to(device)
    print(f"\nğŸ¬ ì…ë ¥ í…ì„œ í¬ê¸°: {input_tensor.shape}")
    
    # ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
    model.eval()
    with torch.no_grad():
        start_time = time.time()
        output = model(input_tensor)
        end_time = time.time()
    
    print(f"   - ì¶œë ¥ í¬ê¸°: {output.shape}")
    print(f"   - ì¶”ë¡  ì‹œê°„: {(end_time - start_time) * 1000:.2f}ms")
    print(f"   - ì¶œë ¥ ë²”ìœ„: [{output.min().item():.4f}, {output.max().item():.4f}]")
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
    if args.benchmark:
        print(f"\nâ±ï¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
        benchmark_results = benchmark_model(model, input_tensor, args.num_runs)
        
        print(f"\nğŸ“ˆ ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼:")
        print(f"   - í‰ê·  ì¶”ë¡  ì‹œê°„: {benchmark_results['mean_time']:.2f} Â± {benchmark_results['std_time']:.2f}ms")
        print(f"   - ìµœì†Œ ì¶”ë¡  ì‹œê°„: {benchmark_results['min_time']:.2f}ms")
        print(f"   - ìµœëŒ€ ì¶”ë¡  ì‹œê°„: {benchmark_results['max_time']:.2f}ms")
        print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {benchmark_results['memory_used']:.2f}MB")
        print(f"   - ì²˜ë¦¬ëŸ‰: {1000 / benchmark_results['mean_time']:.1f} FPS")
    
    # ë°°í¬ ëª¨ë“œ ë³€í™˜ ë°ëª¨
    print(f"\nğŸ”§ ë°°í¬ ëª¨ë“œ ë³€í™˜ ë°ëª¨...")
    try:
        deployed_model = convert_to_deployable_form(model, input_tensor)
        print("âœ… ë°°í¬ ëª¨ë“œ ë³€í™˜ ì„±ê³µ!")
        
        # ë°°í¬ ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        with torch.no_grad():
            start_time = time.time()
            deploy_output = deployed_model(input_tensor)
            end_time = time.time()
        
        print(f"   - ë°°í¬ ëª¨ë¸ ì¶”ë¡  ì‹œê°„: {(end_time - start_time) * 1000:.2f}ms")
        
        # ê²°ê³¼ ë¹„êµ
        max_diff = torch.max(torch.abs(output - deploy_output)).item()
        print(f"   - ì¶œë ¥ ì°¨ì´ (ìµœëŒ€): {max_diff:.6f}")
        
        if args.benchmark:
            print(f"\nâ±ï¸ ë°°í¬ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹...")
            deploy_benchmark = benchmark_model(deployed_model, input_tensor, args.num_runs // 2)
            print(f"   - ë°°í¬ ëª¨ë¸ í‰ê·  ì‹œê°„: {deploy_benchmark['mean_time']:.2f}ms")
            improvement = (benchmark_results['mean_time'] - deploy_benchmark['mean_time']) / benchmark_results['mean_time'] * 100
            print(f"   - ì„±ëŠ¥ í–¥ìƒ: {improvement:.1f}%")
        
    except Exception as e:
        print(f"âŒ ë°°í¬ ëª¨ë“œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        deployed_model = model
    
    # ëª¨ë°”ì¼ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
    if args.export_mobile:
        fp32_path, int8_path = export_mobile_models(deployed_model, input_tensor, args.variant)
        
        if fp32_path:
            # ë‚´ë³´ë‚¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ§ª ë‚´ë³´ë‚¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
            try:
                mobile_model = torch.jit.load(str(fp32_path))
                mobile_model.eval()
                
                with torch.no_grad():
                    mobile_output = mobile_model(input_tensor.cpu())
                
                print(f"âœ… FP32 ëª¨ë°”ì¼ ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ì„±ê³µ!")
                
                if int8_path:
                    int8_model = torch.jit.load(str(int8_path))
                    int8_model.eval()
                    
                    with torch.no_grad():
                        int8_output = int8_model(input_tensor.cpu())
                    
                    print(f"âœ… INT8 ëª¨ë°”ì¼ ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ì„±ê³µ!")
                    
                    # ì–‘ìí™” ì •í™•ë„ ë¹„êµ
                    accuracy_loss = torch.mean(torch.abs(mobile_output - int8_output)).item()
                    print(f"   - ì–‘ìí™”ë¡œ ì¸í•œ ì •í™•ë„ ì†ì‹¤: {accuracy_loss:.6f}")
                
            except Exception as e:
                print(f"âŒ ë‚´ë³´ë‚¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"\nğŸ‰ EfficientX3D-{args.variant} ë°ëª¨ ì™„ë£Œ!")
    
    # ì‚¬ìš© ê°€ì´ë“œ
    print(f"\nğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ:")
    print(f"   ğŸ¯ ìš©ë„: {config['description']}")
    print(f"   ğŸ“± ëª¨ë°”ì¼ í†µí•©: TorchScript ëª¨ë¸ íŒŒì¼ì„ ì•±ì— í¬í•¨")
    print(f"   âš¡ ìµœì í™”: ì¶”ë¡  ì‹œ ë°°í¬ ëª¨ë“œ ìë™ ì‚¬ìš©")
    print(f"   ğŸ—œï¸ ì••ì¶•: INT8 ì–‘ìí™”ë¡œ ~75% í¬ê¸° ê°ì†Œ")
    
    if args.variant == "XS":
        print(f"   ğŸš€ ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•© (< 250ms ì§€ì—°ì‹œê°„)")
    else:
        print(f"   âš–ï¸ ì •í™•ë„ì™€ íš¨ìœ¨ì„±ì˜ ê· í˜• (~700ms ì§€ì—°ì‹œê°„)")


if __name__ == "__main__":
    main() 